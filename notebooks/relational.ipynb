{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gretel Relational\n",
    "Synthetics and Transforms for relational data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client import configure_session\n",
    "\n",
    "configure_session(api_key=\"prompt\", cache=\"yes\", validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end synthetics example\n",
    "\n",
    "from gretel_trainer.relational import MultiTable, sqlite_conn\n",
    "\n",
    "\n",
    "!curl -o \"ecom_xf.db\" \"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecom_xf.db\"\n",
    "\n",
    "\n",
    "connector = sqlite_conn(\"ecom_xf.db\")\n",
    "relational_data = connector.extract()\n",
    "\n",
    "mt = MultiTable(relational_data)\n",
    "mt.train_synthetics(config=\"synthetics/amplify\")\n",
    "mt.generate()\n",
    "\n",
    "connector.save(mt.synthetic_output_tables, prefix=\"synthetic_\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed walkthrough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up source relational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schema of our demo database\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(\"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecommerce_db.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the demo database\n",
    "\n",
    "!curl -o \"ecom_xf.db\" \"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecom_xf.db\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core Python object capturing source relational data and metadata is named `RelationalData`.\n",
    "It can be created automatically using a `Connector`, or it can be created manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database and extract relational data\n",
    "\n",
    "from gretel_trainer.relational import sqlite_conn\n",
    "\n",
    "ecommerce_db_path = \"ecom_xf.db\"\n",
    "\n",
    "sqlite = sqlite_conn(path=ecommerce_db_path)\n",
    "relational_data = sqlite.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, manually define relational data\n",
    "\n",
    "from gretel_trainer.relational import RelationalData\n",
    "import pandas as pd\n",
    "\n",
    "csv_dir = \"/path/to/extracted_csvs\"\n",
    "\n",
    "tables = [\n",
    "    (\"events\", \"id\"),\n",
    "    (\"users\", \"id\"),\n",
    "    (\"distribution_center\", \"id\"),\n",
    "    (\"products\", \"id\"),\n",
    "    (\"inventory_items\", \"id\"),\n",
    "    (\"order_items\", \"id\"),\n",
    "]\n",
    "\n",
    "foreign_keys = [\n",
    "    {\n",
    "        \"table\": \"events\",\n",
    "        \"constrained_columns\": [\"user_id\"],\n",
    "        \"referred_table\": \"users\",\n",
    "        \"referred_columns\": [\"id\"],\n",
    "    },\n",
    "    {\n",
    "        \"table\": \"order_items\",\n",
    "        \"constrained_columns\": [\"user_id\"],\n",
    "        \"referred_table\": \"users\",\n",
    "        \"referred_columns\": [\"id\"],\n",
    "    },\n",
    "    {\n",
    "        \"table\": \"order_items\",\n",
    "        \"constrained_columns\": [\"inventory_item_id\"],\n",
    "        \"referred_table\": \"inventory_items\",\n",
    "        \"referred_columns\": [\"id\"],\n",
    "    },\n",
    "    {\n",
    "        \"table\": \"inventory_items\",\n",
    "        \"constrained_columns\": [\"product_id\"],\n",
    "        \"referred_table\": \"products\",\n",
    "        \"referred_columns\": [\"id\"],\n",
    "    },\n",
    "    {\n",
    "        \"table\": \"inventory_items\",\n",
    "        \"constrained_columns\": [\"product_distribution_center_id\"],\n",
    "        \"referred_table\": \"distribution_center\",\n",
    "        \"referred_columns\": [\"id\"],\n",
    "    },\n",
    "    {\n",
    "        \"table\": \"products\",\n",
    "        \"constrained_columns\": [\"distribution_center_id\"],\n",
    "        \"referred_table\": \"distribution_center\",\n",
    "        \"referred_columns\": [\"id\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "rel_data = RelationalData()\n",
    "\n",
    "for table, pk in tables:\n",
    "    rel_data.add_table(name=table, primary_key=pk, data=pd.read_csv(f\"{csv_dir}/{table}.csv\"))\n",
    "\n",
    "for foreign_key in foreign_keys:\n",
    "    rel_data.add_foreign_key_constraint(**foreign_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how it was created, a `RelationalData` instance can be modified after creation if necessary. In addition to the methods in the manual example above, you can modify source table data, change primary keys, and remove foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't actually want to make these changes; they just serve as examples\n",
    "\n",
    "# Overwrite source data with a different dataframe\n",
    "# rel_data.update_table_data(table=\"users\", data=pd.read_csv(\"alt_users.csv\"))\n",
    "\n",
    "# Change which column (if any) is designated as the primary key\n",
    "# rel_data.set_primary_key(table=\"distribution_center\", primary_key=\"name\")\n",
    "# rel_data.set_primary_key(table=\"order_items\", primary_key=None)\n",
    "\n",
    "# Remove a foreign key relationship\n",
    "# rel_data.remove_foreign_key(\"inventory_items.product_distribution_center_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operate on the source data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MultiTable` class is the interface to working with relational data. It requires a `RelationalData` instance. Several other options can be configured; the defaults are shown below as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_trainer.relational import MultiTable\n",
    "\n",
    "multitable = MultiTable(\n",
    "    relational_data,\n",
    "    # project_display_name=\"multi-table\",\n",
    "    # strategy=\"independent\",\n",
    "    # refresh_interval=60,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify\n",
    "\n",
    "Run Gretel Classify on all tables to identify PII.\n",
    "\n",
    "By default, Relational Classify will provide results for the first 100 rows in each table. To process the entire table, set `all_rows=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "schema_version: \"1.0\"\n",
    "name: \"classify-default\"\n",
    "models:\n",
    "  - classify:\n",
    "      data_source: \"_\"\n",
    "      labels:\n",
    "        - person_name\n",
    "        - credit_card_number\n",
    "        - phone_number\n",
    "        - us_social_security_number\n",
    "        - email_address\n",
    "        - location\n",
    "        - acme/*\n",
    "        \n",
    "label_predictors:\n",
    "  namespace: acme\n",
    "  regex:\n",
    "    user_id:\n",
    "      patterns:\n",
    "        - score: high\n",
    "          regex: ^user_[\\d]{5}$\n",
    "\"\"\"\n",
    "config = yaml.safe_load(config_yaml)\n",
    "\n",
    "multitable.classify(config)\n",
    "\n",
    "# Run classify on all rows\n",
    "# multitable.classify(config, all_rows=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n",
    "\n",
    "Train Gretel Transforms models by providing a transforms model config. By default this config will be applied to all tables. You can limit the tables being transformed via the optional `only` (tables to include) or `ignore` (tables to exclude) arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"https://raw.githubusercontent.com/gretelai/gdpr-helpers/main/src/config/transforms_config.yaml\"\n",
    "\n",
    "multitable.train_transforms(config)\n",
    "\n",
    "# Optionally limit which tables are trained for transforms via `only` (included) or `ignore` (excluded).\n",
    "# Given our example data, the two calls below will lead to the same tables getting trained, just specified different ways.\n",
    "#\n",
    "# multitable.train_transforms(config, ignore={\"distribution_center\", \"products\"})\n",
    "# multitable.train_transforms(config, only={\"users\", \"events\", \"inventory_items\", \"order_items\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run transforms to get transformed output. Each call to `run_transforms` is assigned (or supplied) a unique identifier; look for the transformed output tables in a subdirectory matching that identifier name in the working directory. An archive file containing all runs' outputs is also uploaded to the Gretel project as a project artifact, visible in the Data Sources tab in the Console.\n",
    "\n",
    "By default, `run_transforms` operates on the original source data for all tables with successfully completed transforms models.\n",
    "\n",
    "You can optionally run other data through transforms by passing it in as Pandas DataFrames to the optional `data` argument. In this case, only the provided tables will be transformed (not _all_ tables as in the default, no-`data`-argument case).\n",
    "\n",
    "If you intend to train synthetic models on the transformed output instead of the original source data, add the argument `in_place=True`. **This will modify the data in the `RelationalData` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitable.run_transforms()\n",
    "\n",
    "# Provide a specific identifier for the run (default is `transforms_{timestamp}`)\n",
    "# multitable.run_transforms(identifier=\"my-transforms-run\")\n",
    "\n",
    "# Overwrite source data so that future synthetics actions consider the transformed output as the source\n",
    "# multitable.run_transforms(in_place=True)\n",
    "\n",
    "# Run other data through the trained transforms models\n",
    "# multitable.run_transforms(data={\"events\": some_other_events_dataframe})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original to transformed\n",
    "\n",
    "print(multitable.relational_data.get_table_data(\"users\").head(5))\n",
    "print(multitable.transform_output_tables[\"users\"].head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by training models for synthetics. By default, a synthetics model will be trained for every table in the `RelationalData`. However, this scope can be reduced to a subset of tables using the optional `only` (tables to include) or `ignore` (tables to exclude) arguments. This can be particularly useful if certain tables contain static reference data that should not be synthesized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train synthetic models for all tables\n",
    "\n",
    "multitable.train_synthetics(config=\"synthetics/amplify\")\n",
    "\n",
    "# Optionally limit which tables are trained for synthetics via `only` (included) or `ignore` (excluded).\n",
    "# Given our example data, the two calls below will lead to the same tables getting trained, just specified different ways.\n",
    "#\n",
    "# multitable.train_synthetics(config=\"synthetics/amplify\", ignore={\"distribution_center\", \"products\"})\n",
    "# multitable.train_synthetics(config=\"synthetics/amplify\", only={\"users\", \"events\", \"inventory_items\", \"order_items\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training is complete, you'll find a number of artifacts in your working directory, including the CSVs on which models were trained (`synthetics_train_{table}.csv`) and evaluation reports (`synthetics_[type]_evaluation_{table}.[html|json]`).\n",
    "\n",
    "You can also view some evaluation metrics at this point. (We'll expand upon them after generating synthetic data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitable.evaluations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synthetic data generation run is assigned (or supplied) a unique identifier. Look for a subdirectory with this identifier name in the working directory to find all synthetic outputs, including data and reports. An archive file containing all runs' outputs is also uploaded to the Gretel project as a project artifact, visible in the Data Sources tab in the Console.\n",
    "\n",
    "When you generate synthetic data, you can optionally change the amount of data to generate via `record_size_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "multitable.generate()\n",
    "\n",
    "# Provide a specific identifier for the run (default is `synthetics_{timestamp}`)\n",
    "# multitable.generate(identifier=\"my-synthetics-run\")\n",
    "\n",
    "# Generate twice as much synthetic data\n",
    "# multitable.generate(record_size_ratio=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original to synthetic data\n",
    "\n",
    "print(multitable.relational_data.get_table_data(\"users\").head(5))\n",
    "print(multitable.synthetic_output_tables[\"users\"].head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take another look at our evaluations, we'll see additional metrics are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitable.evaluations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also automatically generated a full relational report summarizing and explaining all this information. Look for `relational_report.html` in the generate run subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from smart_open import open\n",
    "\n",
    "report_path = str(multitable._working_dir / multitable._synthetics_run.identifier / \"relational_report.html\")\n",
    "\n",
    "IPython.display.HTML(data=open(report_path).read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data is automatically written to the working directory in CSV format as `synth_{table}.csv`. You can optionally use a `Connector` to write the synthetic data to a database. (If you're writing back to the same database as your source, pass a `prefix: str` argument to the `save` method to avoid overwriting your source tables!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output data to a new SQLite database\n",
    "\n",
    "from gretel_trainer.relational import sqlite_conn\n",
    "\n",
    "synthetic_db_path = \"out.db\"\n",
    "\n",
    "synthetic_db_conn = sqlite_conn(synthetic_db_path)\n",
    "synthetic_db_conn.save(multitable.synthetic_output_tables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgres demo via Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up a postgres container with docker\n",
    "\n",
    "!docker run --rm -d --name multitable_pgdemo -e POSTGRES_PASSWORD=password -p 5432:5432 postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write synthetic tables to the Postgres db\n",
    "\n",
    "from gretel_trainer.relational import postgres_conn\n",
    "\n",
    "out_db = postgres_conn(\"postgres\", \"password\", \"localhost\", 5432)\n",
    "out_db.save(multitable.synthetic_output_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the postgres database\n",
    "\n",
    "!docker exec multitable_pgdemo psql -U postgres -c \"\\dt\"\n",
    "!docker exec multitable_pgdemo psql -U postgres -c \"select * from users limit 5;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tear down the docker container\n",
    "\n",
    "!docker stop multitable_pgdemo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8726cf33f00e2373738d19e8a73b26d03723d6c732c72211354be2991192c77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
