{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gretel Relational\n",
    "Synthetics and Transforms for relational data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end synthetics example\n",
    "\n",
    "from gretel_trainer.relational import MultiTable, sqlite_conn\n",
    "\n",
    "\n",
    "!curl -o \"ecom_xf.db\" \"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecom_xf.db\"\n",
    "\n",
    "\n",
    "connector = sqlite_conn(\"ecom_xf.db\")\n",
    "relational_data = connector.extract()\n",
    "\n",
    "mt = MultiTable(relational_data)\n",
    "mt.train()\n",
    "mt.generate()\n",
    "\n",
    "connector.save(mt.synthetic_output_tables, prefix=\"synthetic_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed walkthrough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up source relational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schema of our demo database\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(\"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecommerce_db.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the demo database\n",
    "\n",
    "!curl -o \"ecom_xf.db\" \"https://gretel-blueprints-pub.s3.us-west-2.amazonaws.com/rdb/ecom_xf.db\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core Python object capturing source relational data and metadata is named `RelationalData`.\n",
    "It can be created automatically using a `Connector`, or it can be created manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database and extract relational data\n",
    "\n",
    "from gretel_trainer.relational import sqlite_conn\n",
    "\n",
    "ecommerce_db_path = \"ecom_xf.db\"\n",
    "\n",
    "sqlite = sqlite_conn(path=ecommerce_db_path)\n",
    "relational_data = sqlite.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, manually define relational data\n",
    "\n",
    "from gretel_trainer.relational import RelationalData\n",
    "import pandas as pd\n",
    "\n",
    "csv_dir = \"/path/to/extracted_csvs\"\n",
    "\n",
    "tables = [\n",
    "    (\"events\", \"id\"),\n",
    "    (\"users\", \"id\"),\n",
    "    (\"distribution_center\", \"id\"),\n",
    "    (\"products\", \"id\"),\n",
    "    (\"inventory_items\", \"id\"),\n",
    "    (\"order_items\", \"id\"),\n",
    "]\n",
    "\n",
    "foreign_keys = [\n",
    "    (\"events.user_id\", \"users.id\"),\n",
    "    (\"order_items.user_id\", \"users.id\"),\n",
    "    (\"order_items.inventory_item_id\", \"inventory_items.id\"),\n",
    "    (\"inventory_items.product_id\", \"products.id\"),\n",
    "    (\"inventory_items.product_distribution_center_id\", \"distribution_center.id\"),\n",
    "    (\"products.distribution_center_id\", \"distribution_center.id\"),\n",
    "]\n",
    "\n",
    "rel_data = RelationalData()\n",
    "\n",
    "for table, pk in tables:\n",
    "    rel_data.add_table(name=table, primary_key=pk, data=pd.read_csv(f\"{csv_dir}/{table}.csv\"))\n",
    "\n",
    "for fk, ref in foreign_keys:\n",
    "    rel_data.add_foreign_key(foreign_key=fk, referencing=ref)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how it was created, a `RelationalData` instance can be modified after creation if necessary. In addition to the methods in the manual example above, you can modify source table data, change primary keys, and remove foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't actually want to make these changes; they just serve as examples\n",
    "\n",
    "# Overwrite source data with a different dataframe\n",
    "# rel_data.update_table_data(table=\"users\", data=pd.read_csv(\"alt_users.csv\"))\n",
    "\n",
    "# Change which column (if any) is designated as the primary key\n",
    "# rel_data.set_primary_key(table=\"distribution_center\", primary_key=\"name\")\n",
    "# rel_data.set_primary_key(table=\"order_items\", primary_key=None)\n",
    "\n",
    "# Remove a foreign key relationship\n",
    "# rel_data.remove_foreign_key(\"inventory_items.product_distribution_center_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operate on the source data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MultiTable` class is the interface to working with relational data. It requires a `RelationalData` instance. Several other options can be configured; the defaults are shown below as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_trainer.relational import MultiTable\n",
    "\n",
    "multitable = MultiTable(\n",
    "    relational_data,\n",
    "    # project_display_name=\"multi-table\",\n",
    "    # gretel_model=\"amplify\",\n",
    "    # strategy=\"independent\",\n",
    "    # refresh_interval=60,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n",
    "\n",
    "Train Gretel Transforms models by providing table-specific model configs. You only need to train models for tables you want to transformâ€”you do not need to supply a config for every table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform some tables\n",
    "\n",
    "multitable.train_transform_models(\n",
    "    configs={\n",
    "        \"users\": \"https://gretel-blueprints-pub.s3.amazonaws.com/rdb/users_policy.yaml\",\n",
    "        \"events\": \"https://gretel-blueprints-pub.s3.amazonaws.com/rdb/events_policy.yaml\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run transforms to get transformed output. Each call to `run_transforms` is assigned (or supplied) a unique identifier; look for the transformed output tables in a subdirectory matching that identifier name in the working directory. An archive file containing all runs' outputs is also uploaded to the Gretel project as a project artifact, visible in the Data Sources tab in the Console.\n",
    "\n",
    "By default, `run_transforms` operates on the original source data for all tables with successfully completed transforms models.\n",
    "\n",
    "You can optionally run other data through transforms by passing it in as Pandas DataFrames to the optional `data` argument. In this case, only the provided tables will be transformed (not _all_ tables as in the default, no-`data`-argument case).\n",
    "\n",
    "If you intend to train synthetic models on the transformed output instead of the original source data, add the argument `in_place=True`. **This will modify the data in the `RelationalData` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitable.run_transforms()\n",
    "\n",
    "# Provide a specific identifier for the run (default is `transforms_{timestamp}`)\n",
    "# multitable.run_transforms(identifier=\"my-transforms-run\")\n",
    "\n",
    "# Overwrite source data so that future synthetics actions consider the transformed output as the source\n",
    "# multitable.run_transforms(in_place=True)\n",
    "\n",
    "# Run other data through the trained transforms models\n",
    "# multitable.run_transforms(data={\"events\": some_other_events_dataframe})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original to transformed\n",
    "\n",
    "print(multitable.relational_data.get_table_data(\"users\").head(5))\n",
    "print(multitable.transform_output_tables[\"users\"].head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train synthetic models for all tables\n",
    "\n",
    "multitable.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training is complete, you'll find a number of artifacts in your working directory, including the CSVs on which models were trained (`synthetics_train_{table}.csv`) and evaluation reports (`synthetics_[type]_evaluation_{table}.[html|json]`).\n",
    "\n",
    "You can also view some evaluation metrics at this point. (We'll expand upon them after generating synthetic data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitable.evaluations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synthetic data generation run is assigned (or supplied) a unique identifier. Look for a subdirectory with this identifier name in the working directory to find all synthetic outputs, including data and reports. An archive file containing all runs' outputs is also uploaded to the Gretel project as a project artifact, visible in the Data Sources tab in the Console.\n",
    "\n",
    "When you generate synthetic data, you can optionally change the amount of data to generate via `record_size_ratio`, as well as optionally preserve certain tables' source data via `preserve_tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "multitable.generate()\n",
    "\n",
    "# Provide a specific identifier for the run (default is `synthetics_{timestamp}`)\n",
    "# multitable.generate(identifier=\"my-synthetics-run\")\n",
    "\n",
    "# Generate twice as much synthetic data\n",
    "# multitable.generate(record_size_ratio=2.0)\n",
    "\n",
    "# Treat certain tables as static reference data that should not be synthesized\n",
    "# multitable.generate(preserve_tables=[\"distribution_center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original to synthetic data\n",
    "\n",
    "print(multitable.relational_data.get_table_data(\"users\").head(5))\n",
    "print(multitable.synthetic_output_tables[\"users\"].head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take another look at our evaluations, we'll see additional metrics are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitable.evaluations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also automatically generated a full relational report summarizing and explaining all this information. Look for `relational_report.html` in the generate run subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from smart_open import open\n",
    "\n",
    "report_path = str(multitable._working_dir / multitable._synthetics_run.identifier / \"relational_report.html\")\n",
    "\n",
    "IPython.display.HTML(data=open(report_path).read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data is automatically written to the working directory in CSV format as `synth_{table}.csv`. You can optionally use a `Connector` to write the synthetic data to a database. (If you're writing back to the same database as your source, pass a `prefix: str` argument to the `save` method to avoid overwriting your source tables!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output data to a new SQLite database\n",
    "\n",
    "from gretel_trainer.relational import sqlite_conn\n",
    "\n",
    "synthetic_db_path = \"out.db\"\n",
    "\n",
    "synthetic_db_conn = sqlite_conn(synthetic_db_path)\n",
    "synthetic_db_conn.save(multitable.synthetic_output_tables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgres demo via Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up a postgres container with docker\n",
    "\n",
    "!docker run --rm -d --name multitable_pgdemo -e POSTGRES_PASSWORD=password -p 5432:5432 postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write synthetic tables to the Postgres db\n",
    "\n",
    "from gretel_trainer.relational import postgres_conn\n",
    "\n",
    "out_db = postgres_conn(\"postgres\", \"password\", \"localhost\", 5432)\n",
    "out_db.save(multitable.synthetic_output_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the postgres database\n",
    "\n",
    "!docker exec multitable_pgdemo psql -U postgres -c \"\\dt\"\n",
    "!docker exec multitable_pgdemo psql -U postgres -c \"select * from users limit 5;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tear down the docker container\n",
    "\n",
    "!docker stop multitable_pgdemo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8726cf33f00e2373738d19e8a73b26d03723d6c732c72211354be2991192c77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
